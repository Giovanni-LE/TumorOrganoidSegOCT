{"cells":[{"cell_type":"markdown","metadata":{"id":"17g5BxJoClPB"},"source":["Inserire il path all'interno del quale è contenuto il file .ipynb"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"qH8sGm0sCkbX","executionInfo":{"status":"ok","timestamp":1677173755420,"user_tz":-60,"elapsed":4,"user":{"displayName":"Cont est","userId":"12913621564358958738"}}},"outputs":[],"source":["path = \"/content/drive/MyDrive/CODICE FINALE/\""]},{"cell_type":"markdown","metadata":{"id":"-ySwOH0qULku"},"source":["Unzip del database"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21060,"status":"ok","timestamp":1677173781808,"user":{"displayName":"Cont est","userId":"12913621564358958738"},"user_tz":-60},"id":"oXLon2MKT8rF","outputId":"9e7ea44c-d735-4999-bb08-cc9fe4d3abc3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["Inserire il path all'interno del quale è contenuto il construction set"],"metadata":{"id":"amtm0za9BhG5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"e4Luoe2NUKiT"},"outputs":[],"source":["!unzip \"/content/drive/MyDrive/OCTchallenge_new.zip\" # inserire path completa in cui si trovano i dati"]},{"cell_type":"markdown","metadata":{"id":"vRI75G7uT9YI"},"source":["# Installazione di MONAI ed esportazione delle librerie"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"j4UKveulRSy0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677173969630,"user_tz":-60,"elapsed":82630,"user":{"displayName":"Cont est","userId":"12913621564358958738"}},"outputId":"db344eba-de10-4f0c-8fe4-6ed3554e8b7e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting monai[all]\n","  Downloading monai-1.1.0-202212191849-py3-none-any.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from monai[all]) (1.22.4)\n","Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.8/dist-packages (from monai[all]) (1.13.1+cu116)\n","Requirement already satisfied: tifffile in /usr/local/lib/python3.8/dist-packages (from monai[all]) (2023.2.3)\n","Collecting tensorboardX\n","  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 KB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting optuna\n","  Downloading optuna-3.1.0-py3-none-any.whl (365 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.3/365.3 KB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nibabel in /usr/local/lib/python3.8/dist-packages (from monai[all]) (3.0.2)\n","Collecting fire\n","  Downloading fire-0.5.0.tar.gz (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting openslide-python==1.1.2\n","  Downloading openslide-python-1.1.2.tar.gz (316 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 KB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting cucim>=22.8.1\n","  Downloading cucim-23.2.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pytorch-ignite==0.4.10\n","  Downloading pytorch_ignite-0.4.10-py3-none-any.whl (264 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.1/264.1 KB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ninja\n","  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from monai[all]) (3.5.3)\n","Collecting pydicom\n","  Downloading pydicom-2.3.1-py3-none-any.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting einops\n","  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.8/dist-packages (from monai[all]) (2.11.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from monai[all]) (3.1.0)\n","Collecting imagecodecs\n","  Downloading imagecodecs-2023.1.23-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.2/36.2 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting transformers<4.22\n","  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from monai[all]) (0.14.1+cu116)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from monai[all]) (1.3.5)\n","Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.8/dist-packages (from monai[all]) (4.64.1)\n","Collecting pynrrd\n","  Downloading pynrrd-1.0.0-py2.py3-none-any.whl (19 kB)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from monai[all]) (5.4.8)\n","Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.8/dist-packages (from monai[all]) (0.18.3)\n","Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.8/dist-packages (from monai[all]) (4.4.0)\n","Collecting itk>=5.2\n","  Downloading itk-5.3.0-cp38-cp38-manylinux_2_28_x86_64.whl (8.3 kB)\n","Collecting mlflow\n","  Downloading mlflow-2.1.1-py3-none-any.whl (16.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nni\n","  Downloading nni-2.10-py3-none-manylinux1_x86_64.whl (56.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.0/56.0 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from monai[all]) (6.0)\n","Requirement already satisfied: lmdb in /usr/local/lib/python3.8/dist-packages (from monai[all]) (0.99)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.8/dist-packages (from monai[all]) (4.3.3)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from monai[all]) (7.1.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from pytorch-ignite==0.4.10->monai[all]) (23.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from cucim>=22.8.1->monai[all]) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown>=4.4.0->monai[all]) (1.15.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown>=4.4.0->monai[all]) (4.6.3)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown>=4.4.0->monai[all]) (2.25.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown>=4.4.0->monai[all]) (3.9.0)\n","Collecting itk-segmentation==5.3.0\n","  Downloading itk_segmentation-5.3.0-cp38-cp38-manylinux_2_28_x86_64.whl (16.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting itk-numerics==5.3.0\n","  Downloading itk_numerics-5.3.0-cp38-cp38-manylinux_2_28_x86_64.whl (58.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting itk-io==5.3.0\n","  Downloading itk_io-5.3.0-cp38-cp38-manylinux_2_28_x86_64.whl (25.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.6/25.6 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting itk-registration==5.3.0\n","  Downloading itk_registration-5.3.0-cp38-cp38-manylinux_2_28_x86_64.whl (26.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting itk-filtering==5.3.0\n","  Downloading itk_filtering-5.3.0-cp38-cp38-manylinux_2_28_x86_64.whl (73.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting itk-core==5.3.0\n","  Downloading itk_core-5.3.0-cp38-cp38-manylinux_2_28_x86_64.whl (81.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14.2->monai[all]) (1.4.1)\n","Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14.2->monai[all]) (1.7.3)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14.2->monai[all]) (3.0)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14.2->monai[all]) (2.9.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->monai[all]) (1.4.4)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->monai[all]) (4.38.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->monai[all]) (0.11.0)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->monai[all]) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib->monai[all]) (2.8.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.8->monai[all]) (4.5.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<4.22->monai[all]) (2022.6.2)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from fire->monai[all]) (2.2.0)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->monai[all]) (5.12.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->monai[all]) (0.19.3)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->monai[all]) (22.2.0)\n","Collecting packaging\n","  Downloading packaging-22.0-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.8/dist-packages (from mlflow->monai[all]) (3.4.1)\n","Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from mlflow->monai[all]) (0.4.3)\n","Collecting databricks-cli<1,>=0.8.7\n","  Downloading databricks-cli-0.17.4.tar.gz (82 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.3/82.3 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting importlib-metadata!=4.7.0,<6,>=3.7.0\n","  Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\n","Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.8/dist-packages (from mlflow->monai[all]) (2.11.3)\n","Requirement already satisfied: sqlalchemy<2,>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from mlflow->monai[all]) (1.4.46)\n","Collecting docker<7,>=4.0.0\n","  Downloading docker-6.0.1-py3-none-any.whl (147 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.5/147.5 KB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cloudpickle<3 in /usr/local/lib/python3.8/dist-packages (from mlflow->monai[all]) (2.2.1)\n","Requirement already satisfied: pytz<2023 in /usr/local/lib/python3.8/dist-packages (from mlflow->monai[all]) (2022.7.1)\n","Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.8/dist-packages (from mlflow->monai[all]) (1.0.2)\n","Collecting shap<1,>=0.40\n","  Downloading shap-0.41.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (575 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m575.9/575.9 KB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from mlflow->monai[all]) (3.19.6)\n","Collecting querystring-parser<2\n","  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n","Collecting alembic<2\n","  Downloading alembic-1.9.4-py3-none-any.whl (210 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.5/210.5 KB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gitpython<4,>=2.1.0\n","  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: entrypoints<1 in /usr/local/lib/python3.8/dist-packages (from mlflow->monai[all]) (0.4)\n","Collecting gunicorn<21\n","  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Flask<3 in /usr/local/lib/python3.8/dist-packages (from mlflow->monai[all]) (1.1.4)\n","Requirement already satisfied: pyarrow<11,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from mlflow->monai[all]) (9.0.0)\n","Collecting PythonWebHDFS\n","  Downloading PythonWebHDFS-0.2.3-py3-none-any.whl (10 kB)\n","Collecting json-tricks>=3.15.5\n","  Downloading json_tricks-3.16.1-py2.py3-none-any.whl (27 kB)\n","Requirement already satisfied: typeguard in /usr/local/lib/python3.8/dist-packages (from nni->monai[all]) (2.7.1)\n","Collecting schema\n","  Downloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n","Collecting colorama\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Collecting responses\n","  Downloading responses-0.22.0-py3-none-any.whl (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting websockets>=10.1\n","  Downloading websockets-10.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: prettytable in /usr/local/lib/python3.8/dist-packages (from nni->monai[all]) (3.6.0)\n","Requirement already satisfied: astor in /usr/local/lib/python3.8/dist-packages (from nni->monai[all]) (0.8.1)\n","Collecting cmaes>=0.9.1\n","  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n","Collecting colorlog\n","  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n","Collecting nptyping\n","  Downloading nptyping-2.5.0-py3-none-any.whl (37 kB)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard->monai[all]) (0.38.4)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->monai[all]) (57.4.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->monai[all]) (1.8.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->monai[all]) (2.16.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard->monai[all]) (1.4.0)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard->monai[all]) (1.0.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->monai[all]) (0.6.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard->monai[all]) (0.4.6)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->monai[all]) (1.51.1)\n","Collecting Mako\n","  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyjwt>=1.7.0\n","  Downloading PyJWT-2.6.0-py3-none-any.whl (20 kB)\n","Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from databricks-cli<1,>=0.8.7->mlflow->monai[all]) (3.2.2)\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.8/dist-packages (from databricks-cli<1,>=0.8.7->mlflow->monai[all]) (0.8.10)\n","Collecting requests\n","  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting urllib3>=1.26.0\n","  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting websocket-client>=0.32.0\n","  Downloading websocket_client-1.5.1-py3-none-any.whl (55 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.8/dist-packages (from Flask<3->mlflow->monai[all]) (1.1.0)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->monai[all]) (5.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->monai[all]) (4.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->monai[all]) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->monai[all]) (1.3.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata!=4.7.0,<6,>=3.7.0->mlflow->monai[all]) (3.14.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from Jinja2<4,>=2.11->mlflow->monai[all]) (2.0.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers<4.22->monai[all]) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers<4.22->monai[all]) (2.10)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers<4.22->monai[all]) (3.0.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<2->mlflow->monai[all]) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<2->mlflow->monai[all]) (3.1.0)\n","Requirement already satisfied: numba in /usr/local/lib/python3.8/dist-packages (from shap<1,>=0.40->mlflow->monai[all]) (0.56.4)\n","Collecting slicer==0.0.7\n","  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy<2,>=1.4.0->mlflow->monai[all]) (2.0.2)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prettytable->nni->monai[all]) (0.2.6)\n","Collecting simplejson\n","  Downloading simplejson-3.18.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.5/135.5 KB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests->transformers<4.22->monai[all]) (1.7.1)\n","Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from responses->nni->monai[all]) (0.10.2)\n","Collecting types-toml\n","  Downloading types_toml-0.10.8.5-py3-none-any.whl (4.5 kB)\n","Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.8/dist-packages (from schema->nni->monai[all]) (0.5.5)\n","Collecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->monai[all]) (0.4.8)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba->shap<1,>=0.40->mlflow->monai[all]) (0.39.1)\n","Building wheels for collected packages: openslide-python, fire, databricks-cli\n","  Building wheel for openslide-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openslide-python: filename=openslide_python-1.1.2-cp38-cp38-linux_x86_64.whl size=27187 sha256=3213424d550aa75d3c7fe6a899c89ba18a4f42c830221dfa2356d6434d0af049\n","  Stored in directory: /root/.cache/pip/wheels/54/f7/99/15df0aea11eefca84d990052a0133ead40443e8abe22d18a11\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116949 sha256=7ccb5e7edf2bd789f3a255a2062100f018ab4070816a83d3c1218d87a2c8018e\n","  Stored in directory: /root/.cache/pip/wheels/5b/eb/43/7295e71293b218ddfd627f935229bf54af9018add7fbb5aac6\n","  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for databricks-cli: filename=databricks_cli-0.17.4-py3-none-any.whl size=142894 sha256=abd5cf3f5defa4aaf6880a03b650274196213d49c6eaa800ed434e9b1caf07e3\n","  Stored in directory: /root/.cache/pip/wheels/48/7c/6e/4bf2c1748c7ecf994ca951591de81674ed6bf633e1e337d873\n","Successfully built openslide-python fire databricks-cli\n","Installing collected packages: types-toml, tokenizers, ninja, json-tricks, websockets, websocket-client, urllib3, smmap, slicer, simplejson, schema, querystring-parser, pyjwt, pydicom, packaging, openslide-python, nptyping, Mako, itk-core, importlib-metadata, imagecodecs, gunicorn, fire, einops, cucim, colorlog, colorama, cmaes, tensorboardX, requests, pytorch-ignite, pynrrd, monai, itk-numerics, itk-io, gitdb, alembic, shap, responses, PythonWebHDFS, optuna, itk-filtering, huggingface-hub, gitpython, docker, databricks-cli, transformers, nni, mlflow, itk-segmentation, itk-registration, itk\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Attempting uninstall: packaging\n","    Found existing installation: packaging 23.0\n","    Uninstalling packaging-23.0:\n","      Successfully uninstalled packaging-23.0\n","  Attempting uninstall: importlib-metadata\n","    Found existing installation: importlib-metadata 6.0.0\n","    Uninstalling importlib-metadata-6.0.0:\n","      Successfully uninstalled importlib-metadata-6.0.0\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.25.1\n","    Uninstalling requests-2.25.1:\n","      Successfully uninstalled requests-2.25.1\n","Successfully installed Mako-1.2.4 PythonWebHDFS-0.2.3 alembic-1.9.4 cmaes-0.9.1 colorama-0.4.6 colorlog-6.7.0 cucim-23.2.0 databricks-cli-0.17.4 docker-6.0.1 einops-0.6.0 fire-0.5.0 gitdb-4.0.10 gitpython-3.1.31 gunicorn-20.1.0 huggingface-hub-0.12.1 imagecodecs-2023.1.23 importlib-metadata-5.2.0 itk-5.3.0 itk-core-5.3.0 itk-filtering-5.3.0 itk-io-5.3.0 itk-numerics-5.3.0 itk-registration-5.3.0 itk-segmentation-5.3.0 json-tricks-3.16.1 mlflow-2.1.1 monai-1.1.0 ninja-1.11.1 nni-2.10 nptyping-2.5.0 openslide-python-1.1.2 optuna-3.1.0 packaging-22.0 pydicom-2.3.1 pyjwt-2.6.0 pynrrd-1.0.0 pytorch-ignite-0.4.10 querystring-parser-1.2.4 requests-2.28.2 responses-0.22.0 schema-0.7.5 shap-0.41.0 simplejson-3.18.3 slicer-0.0.7 smmap-5.0.0 tensorboardX-2.6 tokenizers-0.12.1 transformers-4.21.3 types-toml-0.10.8.5 urllib3-1.26.14 websocket-client-1.5.1 websockets-10.4\n"]}],"source":["!pip install 'monai[all]'"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31352,"status":"ok","timestamp":1677174000962,"user":{"displayName":"Cont est","userId":"12913621564358958738"},"user_tz":-60},"id":"tgb4UFbuSGvb","outputId":"facbc635-8763-4a21-d9a8-9838d0c92cab"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pynrrd in /usr/local/lib/python3.8/dist-packages (1.0.0)\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from pynrrd) (1.22.4)\n","Requirement already satisfied: nptyping in /usr/local/lib/python3.8/dist-packages (from pynrrd) (2.5.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from pynrrd) (4.5.0)\n"]}],"source":["!pip install pynrrd\n","\n","import os\n","from monai.inferers import SimpleInferer\n","import nrrd\n","\n","from monai.utils import set_determinism\n","from monai.networks.nets import UNet\n","from monai.networks.layers import Norm\n","from monai.config import print_config\n","from monai.metrics import DiceMetric,ConfusionMatrixMetric\n","\n","import monai\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as mpatches\n","import numpy as np\n","import torch \n","import PIL\n","from PIL import ImageEnhance\n","\n","from tqdm import tqdm\n","from skimage import measure, segmentation\n","from skimage.io import imread, imshow, imsave\n","from skimage.transform import resize\n","\n","from skimage.transform import rotate\n","from scipy.ndimage import binary_fill_holes\n","from sklearn.preprocessing import minmax_scale\n","\n","import skimage.io as io\n","from skimage.measure import label, regionprops, regionprops_table\n","from skimage import morphology,exposure\n","import cv2\n","import pandas as pd\n","\n","import torch\n","from torchvision import transforms\n","from PIL import Image, ImageFilter, ImageEnhance\n","\n","from monai.transforms import (\n","    AsDiscrete,\n","    DataStatsd,\n","    AddChanneld,\n","    Compose,\n","    Activations,\n","    LoadImage,\n","    LoadImaged,\n","    Resize,\n","    Resized,\n","    RandFlipd,\n","    ScaleIntensityRange,\n","    ScaleIntensityRanged,\n","    DataStats,\n","    AsChannelFirstd,\n","    AsDiscreted,\n","    ToTensord,\n","    EnsureType,\n","    ThresholdIntensityd,\n","    SpatialCropd,\n","    CropForegroundd,\n","    EnsureChannelFirstd,\n","    RandSpatialCropSamplesd,\n","    Orientationd,\n","    Spacingd,\n","    DataStatsd,\n","    RandRotate90d,\n","    RandRotated,\n","    RandAxisFlipd,\n",")\n","\n","from monai.data import (\n","    DataLoader,\n","    CacheDataset,\n","    PILReader,\n","    ITKReader,\n","    NrrdReader,\n","    IterableDataset,\n","    decollate_batch,\n",")"]},{"cell_type":"markdown","metadata":{"id":"kuu_l8ExVSIw"},"source":["# Definizione dei path"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Iu0TFlQvVJCy","executionInfo":{"status":"ok","timestamp":1677174000963,"user_tz":-60,"elapsed":8,"user":{"displayName":"Cont est","userId":"12913621564358958738"}}},"outputs":[],"source":["current_dir = os.getcwd()\n","dataset_name = 'new dataset'\n","\n","#Poichè all'interno delle cartelle di training e validation vi sono due sotto-cartelle (w6_d5 e w6_d7), \n","#sono state create rispettivamente due liste ordinate (sia per le immagini che per le maschere) per training e validation.\n","\n","training1_volumes_path = os.path.join(dataset_name,'Volumes','training','w6_d5')\n","training2_volumes_path = os.path.join(dataset_name,'Volumes','training','w6_d7')\n","\n","validation1_volumes_path = os.path.join(dataset_name,'Volumes','validation','w2_d7')\n","validation2_volumes_path = os.path.join(dataset_name,'Volumes','validation','w6_d9')\n","\n","training1_masks_path = os.path.join(dataset_name,'masks','training','w6_d5')\n","training2_masks_path = os.path.join(dataset_name,'masks','training','w6_d7')\n","\n","validation1_masks_path = os.path.join(dataset_name,'masks','validation','w2_d7')\n","validation2_masks_path = os.path.join(dataset_name,'masks','validation','w6_d9')\n","\n","#liste ordinate per le immagini di training --> 4608 elementi\n","training1_volumes = sorted(os.listdir(training1_volumes_path))\n","training1_volumes = training1_volumes[1:] #al fine di eliminare il primo elemento che è \".DS_Store\", ovvero un file nascosto con un formato proprietario creato da macOS \n","training2_volumes = sorted(os.listdir(training2_volumes_path))\n","\n","#liste ordinate per le maschere di training\n","training1_masks = sorted(os.listdir(training1_masks_path))\n","training2_masks = sorted(os.listdir(training2_masks_path))\n","\n","#liste ordinate per le immagini di validazione --> 1024 elementi \n","validation1_volumes = sorted(os.listdir(validation1_volumes_path))\n","validation2_volumes = sorted(os.listdir(validation2_volumes_path))\n","\n","\n","#liste ordinate per le maschere di validazione\n","validation1_masks = sorted(os.listdir(validation1_masks_path))\n","validation2_masks = sorted(os.listdir(validation2_masks_path))\n"]},{"cell_type":"markdown","metadata":{"id":"Ictw2yudbq7E"},"source":["# Creazione del training e validation set dict"]},{"cell_type":"markdown","source":["Creazione del training set dict"],"metadata":{"id":"Gzk5Kw97BxrJ"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"KCWvR5MXaUG4","executionInfo":{"status":"ok","timestamp":1677174000963,"user_tz":-60,"elapsed":7,"user":{"displayName":"Cont est","userId":"12913621564358958738"}}},"outputs":[],"source":["training_data = [] # Inizializzazione della lista di tris di immagini di training\n","for i in range(len(training1_volumes)): #ciclo sul primo set di immagini di training \n","    tempDict = {\n","        'image': os.path.join(training1_volumes_path, training1_volumes[i]),\n","        'segmentation': os.path.join(training1_masks_path, training1_masks[i])}\n","    \n","    training_data.append(tempDict)\n","\n","for i in range(len(training2_volumes)): #ciclo sul secondo set di immagini di training\n","\n","    tempDict = {\n","        'image': os.path.join(training2_volumes_path, training2_volumes[i]),\n","        'segmentation': os.path.join(training2_masks_path,training2_masks[i])}\n","    \n","    training_data.append(tempDict)"]},{"cell_type":"markdown","metadata":{"id":"Cz7ec1Pdbu12"},"source":["Creazione del validation set dict"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Fvf72u3Pbqgy","executionInfo":{"status":"ok","timestamp":1677174000963,"user_tz":-60,"elapsed":7,"user":{"displayName":"Cont est","userId":"12913621564358958738"}}},"outputs":[],"source":["validation_data = [] # Inizializzazione della lista di tris di immagini di validation\n","for i in range(len(validation1_volumes)): #ciclo sul primo set di immagini di validazione\n","\n","    tempDict = {\n","        'image': os.path.join(validation1_volumes_path, validation1_volumes[i]),\n","        'segmentation': os.path.join(validation1_masks_path, validation1_masks[i])}\n","    \n","    validation_data.append(tempDict)\n","\n","for i in range(len(validation2_volumes)): #ciclo sul secondo set di immagini di validazione\n","    tempDict = {\n","        'image': os.path.join(validation2_volumes_path, validation2_volumes[i]),\n","        'segmentation': os.path.join(validation2_masks_path,validation2_masks[i])}\n","    \n","    validation_data.append(tempDict)"]},{"cell_type":"markdown","metadata":{"id":"DlrkomGX430P"},"source":["# Pre- e Post- processing"]},{"cell_type":"markdown","metadata":{"id":"HC-3C9wp8Kww"},"source":["Definizione di una funzione di pre processing\n","\n","x: immagine 2D originale\n","y: maschera manuale associata all'immagine x\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"EMFj7GU96RMG","executionInfo":{"status":"ok","timestamp":1677174000964,"user_tz":-60,"elapsed":8,"user":{"displayName":"Cont est","userId":"12913621564358958738"}}},"outputs":[],"source":["from scipy.ndimage import median_filter\n","from scipy.ndimage import gaussian_filter\n","from skimage import io\n","from skimage.exposure import rescale_intensity\n","\n","def preprocessing(x,y): #x:immagine, y:segmentazione\n","\n","  #Eliminazione del canale --> (256,256)\n","  x = x.squeeze()\n","  y = y.squeeze()\n","\n","  #Conversione delle immagini in formato numpy\n","  x = x.cpu().numpy()\n","  y = y.cpu().numpy()\n","\n","  return x,y"]},{"cell_type":"markdown","metadata":{"id":"V4kbsuhe0df1"},"source":["Definizione di una funzione di post processing\n","\n","x: maschera automatica in output al modello\n","y: maschera manuale associata all'immagine z\n","z: immagine 2D pre-processata"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"3Vv2cMPo0ftf","executionInfo":{"status":"ok","timestamp":1677174000964,"user_tz":-60,"elapsed":7,"user":{"displayName":"Cont est","userId":"12913621564358958738"}}},"outputs":[],"source":["def postprocessing(x,y,z):\n","\n","  #Conversione delle immagini in formato numpy\n","  x = x.cpu().numpy() \n","  y = y.cpu().numpy()\n","  z = z.cpu().numpy()\n","\n","  return x,y,z"]},{"cell_type":"markdown","metadata":{"id":"8nYuKeHYi4u3"},"source":["# Creazione dei Dataset e DataLoader per le fasi di Training e Validation"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"-yM9TUKLe35F","executionInfo":{"status":"ok","timestamp":1677174000964,"user_tz":-60,"elapsed":7,"user":{"displayName":"Cont est","userId":"12913621564358958738"}}},"outputs":[],"source":["train_transforms = Compose(\n","    [\n","        LoadImaged(keys=[\"image\",\"segmentation\"],image_only=False, reader=ITKReader()), # caricamento dell'immagine e della segmentazione\n","        EnsureChannelFirstd(keys=[\"image\",\"segmentation\"]), # aggiunge il canale all' immagine e alla segmentazione ottenendo il formato channel-first da (NxN) a (1xNxN)\n","        Resized(keys=[\"image\", \"segmentation\"], spatial_size=[256,256]), # resize dell'immagine a 256x256\n","        AsDiscreted(keys=[\"segmentation\"],threshold=0.5), # porta la segmentazione in formato binario con soglia 0.5\n","     \n","        # Operazioni di data augmentation: \n","        RandRotated(keys=[\"image\", \"segmentation\"], range_x=[-0.1, 0.1], prob=0.3, padding_mode=\"zeros\"),\n","    \n","        #DataStatsd(keys = ['image', 'segmentation']), #Per verificare le dimensioni finali dell'immagine\n","        ToTensord(keys=[\"image\", \"segmentation\"]), # per portare immagini e segmentazioni da formato PIL a un torch tensor da dare in input alla rete\n","    ]\n",")\n","\n","val_transforms = Compose(\n","    [\n","        LoadImaged(keys=[\"image\",\"segmentation\"],image_only=False, reader=ITKReader()), # caricamento dell'immagine e della segmentazione\n","        EnsureChannelFirstd(keys=[\"image\",\"segmentation\"]), # aggiunge il canale all' immagine e alla segmentazione ottenendo il formato channel-first da (NxN) a (1xNxN)\n","        Resized(keys=[\"image\", \"segmentation\"], spatial_size=[256,256]), # resize dell'immagine a 256x256\n","        AsDiscreted(keys=[\"segmentation\"],threshold=0.5), # porta la segmentazione in formato binario con soglia 0.5 (abbiamo modificato il range di valori nella trasformazione precedente)\n","        ToTensord(keys=[\"image\", \"segmentation\"]), # per portare immagini e segmentazioni da formato PIL a un torch tensor da dare in input alla rete\n","    ]\n",")"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"7b8kOWioiq_L","executionInfo":{"status":"ok","timestamp":1677174000965,"user_tz":-60,"elapsed":8,"user":{"displayName":"Cont est","userId":"12913621564358958738"}}},"outputs":[],"source":["batch_size= 32\n","\n","# creazione di un oggetto iterabile da cui possiamo estrarre in maniera sequenziale dei dati\n","#  in input forniamo le coppie immagine-segmentazione e le trasformazioni da applicare\n","train_ds = IterableDataset(data = training_data, transform = train_transforms)\n","\n","wks = 0 # definisce il numero di workers (core della CPU da utilizzare) per caricare le immagini\n","\n","# definizione dell'oggetto Dataloader che si occuperà di caricare le immagini in maniera sequenziale\n","# definendo i vari batch durante l'allenamento. Definiamo ora la Batch Size da utilizzare\n","train_loader = DataLoader(train_ds, batch_size=batch_size, num_workers=wks, pin_memory=True, shuffle=False)\n","\n","#Facciamo lo stesso per il validation\n","val_ds = IterableDataset(data = validation_data, transform = val_transforms)\n","val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=wks, pin_memory=True, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"74dWE_JN7_zC"},"source":["# ANALISI TRAIN"]},{"cell_type":"markdown","metadata":{"id":"q5gLj1vb9ZPJ"},"source":["Codice per generare un **dizionario** a partire da dati del **training set**. Da qui si possono ricavare informazioni come l'istogramma dei singoli tumori, la loro area, densità ecc."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":271452,"status":"ok","timestamp":1677174331030,"user":{"displayName":"Cont est","userId":"12913621564358958738"},"user_tz":-60},"id":"U2rQwNNT8Fgw","outputId":"ad43b2bc-6972-479c-c450-a3a376ebe2c4"},"outputs":[{"output_type":"stream","name":"stderr","text":["Pre-processing Train (288 / 288 Steps): 100%|██████████| 288/288 [04:31<00:00,  1.06it/s]\n"]}],"source":["eval_num = int(len(training_data)/batch_size) # definire numero di iterazioni per effettuare validazione ogni epoca \n","max_iterations = eval_num # numero massimo di batch da iterare\n","global_step = 1\n","\n","x_3D= ()\n","y_3D=()\n","\n","epoch_iterator = tqdm(train_loader, desc=\"Pre-processing (X / X Steps)\", dynamic_ncols=True, total=eval_num, position=0, leave=True) # crea un oggetto tqdm per tracciare l'avenzamento delle epoche\n","\n","# Inizializza un dizionario per memorizzare le aree e le densità dei tumori\n","tumor = {}\n","\n","# Contatore per le chiavi univoche\n","counter = 1\n","slice_=1\n","\n","for step, batch in enumerate(epoch_iterator): # ad ogni ciclo for estrae una coppia step (numero dell'iterazione), batch (immagine e segmentazione) dal train loader\n","      \n","    x1, y1 = (batch[\"image\"], batch[\"segmentation\"]) # manda i tensori di immagine e maschera alla GPU --> (32,1,256,256)\n","\n","    for i in range(batch_size):\n","\n","      x2,y2 = x1[i,:,:,:], y1[i,:,:,:]  #Estrazione del primo elemento del batch --> Metatensor (1,256,256)\n","\n","      #Applicazione della funzione pre-processing\n","      x2,y2 = preprocessing(x2,y2) #--> x2/y2.size (256,256)\n","      \n","      image_name = batch['image_meta_dict']['filename_or_obj'][-1][:-5] # Info sull'immagine e la slice\n","      image_mask= x2*y2 #Maschera sovrapposta all'immagine\n","      regions = measure.regionprops(label(y2),image_mask) # Calcola le proprietà delle regioni\n","    \n","      for region in regions: # Per ogni regione (tumore), calcola\n","          area = region.area #area\n","\n","          tumor_coords = region.coords # Estrae le coordinate dei pixel all'interno della regione del tumore          \n","          tumor_norm = minmax_scale(x2[tumor_coords[:, 0], tumor_coords[:, 1]], feature_range=(0, 255)) # Normalizza solo i pixel del tumore\n","          hist, bins = np.histogram(tumor_norm, bins=256, range=(0, 255)) # Calcola l'istogramma della regione normalizzata del tumore\n","          \n","          intensity_mean = np.mean(image_mask[region.coords[:,0], region.coords[:,1]])\n","\n","          tumor[counter] = {'filename': image_name, 'slice': slice_,'area': round(area,1),'intensità media':intensity_mean,'hist': hist }\n","          counter += 1 # Incrementa il contatore\n","\n","      if slice_ ==256:\n","        slice_=0\n","        \n","      slice_ +=1 \n","        \n","    epoch_iterator.set_description(\"Pre-processing Train (%d / %d Steps)\" % (global_step, max_iterations)) # aggiorna il counter tqdm\n","    global_step += 1\n"]},{"cell_type":"markdown","metadata":{"id":"1ObDJjbOYU3N"},"source":["# ANALISI VALIDATION"]},{"cell_type":"markdown","metadata":{"id":"mIUcfCqF90EL"},"source":["Codice per generare un **dizionario** a partire dai dati del **validation set** . Da qui si possono ricavare informazioni come l'istogramma dei singoli tumori, la loro area, densità ecc."]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53634,"status":"ok","timestamp":1677174384657,"user":{"displayName":"Cont est","userId":"12913621564358958738"},"user_tz":-60},"id":"ALuz1z6xYW9B","outputId":"29eb85ff-11a3-45ee-9a89-12d8209494ab"},"outputs":[{"output_type":"stream","name":"stderr","text":["Pre-processing Val (64 / 64 Steps): 100%|██████████| 64/64 [00:52<00:00,  1.21it/s]\n"]}],"source":["eval_num = int(len(validation_data)/batch_size) # definire numero di iterazioni per effettuare validazione ogni epoca \n","max_iterations = eval_num # numero massimo di batch da iterare\n","global_step = 1\n","\n","epoch_iterator = tqdm(val_loader, desc=\"Pre-processing (X / X Steps)\", dynamic_ncols=True, total=eval_num, position=0, leave=True) # crea un oggetto tqdm per tracciare l'avenzamento delle epoche\n","\n","# Inizializza un dizionario per memorizzare le aree e le densità dei tumori\n","tumor_val = {}\n","\n","# Contatore per le chiavi univoche\n","counter = 1\n","slice_=1\n","\n","for step, batch in enumerate(epoch_iterator): # ad ogni ciclo for estrae una coppia step (numero dell'iterazione), batch (immagine e segmentazione) dal train loader\n","      \n","    x1, y1 = (batch[\"image\"], batch[\"segmentation\"]) # manda i tensori di immagine e maschera alla GPU --> (32,1,256,256)\n","\n","    for i in range(batch_size):\n","\n","      x2,y2 = x1[i,:,:,:], y1[i,:,:,:]  #Estrazione del primo elemento del batch --> Metatensor (1,256,256)\n","\n","      #Applicazione della funzione pre-processing\n","      x2,y2 = preprocessing(x2,y2) #--> x2/y2.size (256,256)\n","\n","      image_name = batch['image_meta_dict']['filename_or_obj'][-1][-18:-8] # Info sull'immagine e la slice\n","      image_mask= x2*y2 #Maschera sovrapposta all'immagine\n","      regions = measure.regionprops(label(y2),image_mask) # Calcola le proprietà delle regioni\n","\n","      for region in regions: # Per ogni regione (tumore), calcola l'area e la densità\n","          area = region.area #Area\n","          \n","          tumor_coords = region.coords # Estrae le coordinate dei pixel all'interno della regione del tumore          \n","          tumor_norm = minmax_scale(x2[tumor_coords[:, 0], tumor_coords[:, 1]], feature_range=(0, 255)) # Normalizza solo i pixel del tumore\n","          hist, bins = np.histogram(tumor_norm, bins=256, range=(0, 255)) # Calcola l'istogramma della regione normalizzata del tumore\n","          \n","          intensity_mean = np.mean(image_mask[region.coords[:,0], region.coords[:,1]])\n","\n","          tumor_val[counter] = {'filename': image_name, 'slice': slice_,'area': round(area,1),'intensità media':intensity_mean,'hist': hist }\n","          counter += 1 # Incrementa il contatore\n","\n","      if slice_ ==256:\n","        slice_=0\n","\n","      slice_ +=1 \n","        \n","    epoch_iterator.set_description(\"Pre-processing Val (%d / %d Steps)\" % (global_step, max_iterations)) # aggiorna il counter tqdm\n","    global_step += 1"]},{"cell_type":"markdown","metadata":{"id":"QyMDTSw1-XkS"},"source":["# SALVATAGGIO/CARICAMENTO DEI DIZIONARI"]},{"cell_type":"markdown","metadata":{"id":"tExgeHgcvVgp"},"source":["SALVATAGGIO DEL DIZIONARIO"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"Na7m6B5YurVL","executionInfo":{"status":"ok","timestamp":1677174384658,"user_tz":-60,"elapsed":15,"user":{"displayName":"Cont est","userId":"12913621564358958738"}}},"outputs":[],"source":["import pickle\n","\n","with open(\"tumor.pickle\", \"wb\") as f:\n","  pickle.dump(tumor, f)\n","\n","with open(\"tumor_val.pickle\", \"wb\") as f:\n","  pickle.dump(tumor_val, f)"]},{"cell_type":"markdown","metadata":{"id":"ikCoZk_GvZ-l"},"source":["LOAD DEL DIZIONARIO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zgOoBpU4vFZB"},"outputs":[],"source":["import pickle\n","\n","with open(path + \"tumor.pickle\", \"rb\") as f:\n","  tumor = pickle.load(f)\n","\n","with open(path + \"tumor_val.pickle\", \"rb\") as f:\n","  tumor_val = pickle.load(f)"]},{"cell_type":"markdown","source":["# TABELLA RIASSUNTIVA"],"metadata":{"id":"r86N-cHmFPVF"}},{"cell_type":"code","source":["# Creazione dei DataFrame di Pandas\n","df_train = pd.DataFrame.from_dict(tumor, orient='index')\n","df_val = pd.DataFrame.from_dict(tumor_val, orient='index')\n","\n","# Aggiunta della colonna \"dataset\"\n","df_train['dataset'] = 'training set'\n","df_val['dataset'] = 'validation set'\n","\n","# Unione dei due DataFrame\n","df = pd.concat([df_train, df_val])\n","\n","# Raggruppamento per dataset\n","df_grouped = df.groupby('dataset')\n","\n","# Calcolo delle statistiche per ogni gruppo\n","df_stats = pd.DataFrame({\n","    'area minima': df_grouped['area'].min(),\n","    'area massima': df_grouped['area'].max(),\n","    'intensità minima': df_grouped['intensità media'].min(),\n","    'intensità massima': df_grouped['intensità media'].max()\n","})\n","\n","# Visualizzazione del DataFrame\n","print(df_stats)"],"metadata":{"id":"1aKU_zlsFSfo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677174394486,"user_tz":-60,"elapsed":240,"user":{"displayName":"Cont est","userId":"12913621564358958738"}},"outputId":"d988609b-df96-43ba-a218-ae87b1275665"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["                area minima  area massima  intensità minima  intensità massima\n","dataset                                                                       \n","training set              1          6363        323.322357       12235.237305\n","validation set            1         10336        416.000000       17295.644531\n"]}]}],"metadata":{"colab":{"collapsed_sections":["17g5BxJoClPB","-ySwOH0qULku","vRI75G7uT9YI","kuu_l8ExVSIw","Ictw2yudbq7E","DlrkomGX430P","8nYuKeHYi4u3","1ObDJjbOYU3N","QyMDTSw1-XkS","r86N-cHmFPVF"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}